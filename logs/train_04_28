=> loading checkpoint '/dataset/weights/final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'
=> loaded checkpoint '/dataset/weights/final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23' (epoch 24, bce_best 0.1723601172585609)
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
real 5954 fakes 6279 mode val
real 51912 fakes 51912 mode train
training epoch 24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.3484053773082819, Min Loss: 0.09268341812390858
Epoch 24 improved from 100 to 0.3484053773082819
Epoch: 24 bce: 0.3484053773082819, bce_best: 0.3484053773082819
real 51912 fakes 51912 mode train
training epoch 25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.41615473274390524, Min Loss: 0.05531041333761272
Epoch: 25 bce: 0.41615473274390524, bce_best: 0.3484053773082819
real 51912 fakes 51912 mode train
training epoch 26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.4123542917624702, Min Loss: 0.07802547696339197
Epoch: 26 bce: 0.4123542917624702, bce_best: 0.3484053773082819
real 51912 fakes 51912 mode train
training epoch 27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.37519231737876235, Min Loss: 0.10549969237568242
Epoch: 27 bce: 0.37519231737876235, bce_best: 0.3484053773082819
real 51912 fakes 51912 mode train
training epoch 28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.28783562979665184, Min Loss: 0.13405057612222201
Epoch 28 improved from 0.3484053773082819 to 0.28783562979665184
Epoch: 28 bce: 0.28783562979665184, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.39250780405341923, Min Loss: 0.12097413029613378
Epoch: 29 bce: 0.39250780405341923, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.3146762686571718, Min Loss: 0.2188326330224094
Epoch: 30 bce: 0.3146762686571718, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.3820739096481765, Min Loss: 0.10931636071968912
Epoch: 31 bce: 0.3820739096481765, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 32
Test phase
Max Loss: 0.5114626795682645, Min Loss: 0.16336068395192885
Epoch: 32 bce: 0.5114626795682645, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.44303627401502244, Min Loss: 0.1255888128915123
Epoch: 33 bce: 0.44303627401502244, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 34
Test phase
Max Loss: 0.5599191070551852, Min Loss: 0.18284438690221255
Epoch: 34 bce: 0.5599191070551852, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.5112902616366855, Min Loss: 0.1465715134096919
Epoch: 35 bce: 0.5112902616366855, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 36
Test phase
Max Loss: 0.37686189800520536, Min Loss: 0.20582408593905308
Epoch: 36 bce: 0.37686189800520536, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.4852196800480721, Min Loss: 0.15843099716454506
Epoch: 37 bce: 0.4852196800480721, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.48454025976349224, Min Loss: 0.15826872244080517
Epoch: 38 bce: 0.48454025976349224, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 39
Test phase
Max Loss: 0.47802058184213475, Min Loss: 0.1730193310278603
Epoch: 39 bce: 0.47802058184213475, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.5588683434705357, Min Loss: 0.05265596369886127
Epoch: 40 bce: 0.5588683434705357, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.44646793263253703, Min Loss: 0.17960498318196072
Epoch: 41 bce: 0.44646793263253703, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.5988392701064571, Min Loss: 0.2560910417817386
Epoch: 42 bce: 0.5988392701064571, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 43
Test phase
Max Loss: 0.8175561003547949, Min Loss: 0.07982162025606887
Epoch: 43 bce: 0.8175561003547949, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 1.0324404267247247, Min Loss: 0.2710042030130825
Epoch: 44 bce: 1.0324404267247247, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 45
Test phase
Max Loss: 0.8056398946646697, Min Loss: 0.20807832402293078
Epoch: 45 bce: 0.8056398946646697, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.9284935054428601, Min Loss: 0.30926386387380517
Epoch: 46 bce: 0.9284935054428601, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.665480601786751, Min Loss: 0.3316047440081997
Epoch: 47 bce: 0.665480601786751, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 1.1328559363421693, Min Loss: 0.23989284071640574
Epoch: 48 bce: 1.1328559363421693, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 1.0061706828509824, Min Loss: 0.3472016240669612
Epoch: 49 bce: 1.0061706828509824, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 50
Test phase
Max Loss: 1.2453317161266921, Min Loss: 0.23724711965746473
Epoch: 50 bce: 1.2453317161266921, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.842714420562478, Min Loss: 0.17390635668007362
Epoch: 51 bce: 0.842714420562478, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.829568906310181, Min Loss: 0.13958424089232693
Epoch: 52 bce: 0.829568906310181, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 1.3093311138854364, Min Loss: 0.26858955345513447
Epoch: 53 bce: 1.3093311138854364, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 54
Test phase
Max Loss: 0.8206386208579758, Min Loss: 0.3046951248851701
Epoch: 54 bce: 0.8206386208579758, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.9703052736942609, Min Loss: 0.4513941286050877
Epoch: 55 bce: 0.9703052736942609, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.8840985563883575, Min Loss: 0.35226827747903317
Epoch: 56 bce: 0.8840985563883575, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 57
Test phase
Max Loss: 1.016178803460849, Min Loss: 0.40149392300308906
Epoch: 57 bce: 1.016178803460849, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Test phase
Max Loss: 0.8600866493022246, Min Loss: 0.2774472358388395
Epoch: 58 bce: 0.8600866493022246, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 59
Test phase
Max Loss: 0.9083062069701077, Min Loss: 0.3496588317314951
Epoch: 59 bce: 0.9083062069701077, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 1.1652950616618472, Min Loss: 0.5099119147820504
Epoch: 60 bce: 1.1652950616618472, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.7948156706252465, Min Loss: 0.40476262519354306
Epoch: 61 bce: 0.7948156706252465, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 1.8630323071109125, Min Loss: 0.502820096388334
Epoch: 62 bce: 1.8630323071109125, bce_best: 0.28783562979665184
real 51912 fakes 51912 mode train
training epoch 63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Test phase
Max Loss: 0.8231049597665042, Min Loss: 0.13401841429698344
Epoch: 63 bce: 0.8231049597665042, bce_best: 0.28783562979665184
